---
layout: post
title: PageRank
---

In this section, we study PageRank which is a method for ranking webpages by importance using link structure in the web graph. This is a commonly used [algorithm](http://ilpubs.stanford.edu:8090/422/) for web search popularized by Google. Before discussing PageRank, let us first conceptualize the web as a graph and attempt to study its structure using the language of graph theory.

PageRankæ˜¯ä¸€ç§ä½¿ç”¨Webå›¾ä¸­çš„é“¾æ¥ç»“æ„ï¼ŒæŒ‰é‡è¦æ€§å¯¹ç½‘é¡µè¿›è¡Œæ’åçš„æ–¹æ³•ã€‚ç”±Googleå‘æ‰¬å…‰å¤§çš„ç®—æ³•ï¼Œè¢«å¹¿æ³›ç”¨äºç½‘é¡µæœç´¢å¸¸ç”¨ã€‚ åœ¨è®¨è®ºPageRankä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå°†webæ¦‚å¿µåŒ–ä¸ºå›¾ï¼Œç„¶åå°è¯•ä½¿ç”¨å›¾è®ºè¯­è¨€ç ”ç©¶å…¶ç»“æ„ã€‚

## The web as a graph

We can represent the entire web as a graph by letting the web pages be nodes and the hyperlinks connecting them be directed edges. We will also make a set of simplifying assumptions:

å¯ä»¥é€šè¿‡å°†ç½‘é¡µä½œä¸ºèŠ‚ç‚¹å¹¶å°†è¿æ¥å®ƒä»¬çš„è¶…é“¾æ¥ä½œä¸ºæœ‰å‘è¾¹æ¥å°†æ•´ä¸ªç½‘ç»œè¡¨ç¤ºä¸ºå›¾ã€‚ ä¸€ç»„ç®€åŒ–çš„å‡è®¾ï¼š

- Consider only static webpages
- Ignore dark matter on the web (i.e. inaccessible material, pages behind firewalls)
- All links are navigable. Transactional links (eg: like, buy, follow etc.) are not considered
- ä»…è€ƒè™‘é™æ€ç½‘é¡µ
- å¿½ç•¥ç½‘ç»œä¸Šçš„æš—ç‰©è´¨ï¼ˆå³æ— æ³•è®¿é—®çš„ææ–™ï¼Œè¢«é˜²ç«å¢™æ‹¦æˆªçš„é¡µé¢ï¼‰
- åªè€ƒè™‘å¯æµè§ˆçš„ç½‘é¡µè¿æ¥ã€‚ ä¸è€ƒè™‘transactionalçš„é“¾æ¥ï¼ˆä¾‹å¦‚ï¼šå–œæ¬¢ï¼Œè´­ä¹°ï¼Œå…³æ³¨ç­‰ï¼Œå³ä¸€ç§åŠ¨ä½œã€‚ï¼‰

Conceptualizing the web this way is similar to how some popular search engines view it. For instance, Google indexes web pages using crawlers that explore the web by following links in a breadth-first order. Some other examples of graphs that can be viewed this way are: the graph of citations between scientific research papers, references in an encyclopedia.

ä»¥è¿™ç§æ–¹å¼æ¦‚å¿µåŒ–ç½‘ç»œï¼Œç±»ä¼¼äºæŸäº›æµè¡Œçš„æœç´¢å¼•æ“ã€‚ä¾‹å¦‚ï¼ŒGoogleä½¿ç”¨çˆ¬è™«ä¸ºç½‘é¡µç¼–åˆ¶ç´¢å¼•ï¼Œè¿™äº›çˆ¬è™«é€šè¿‡å¹¿åº¦ä¼˜å…ˆé¡ºåºè®¿é—®é“¾æ¥æ¥æµè§ˆç½‘ç»œã€‚å¯ä»¥é€šè¿‡è¿™ç§æ–¹å¼æŸ¥çœ‹çš„å…¶ä»–ä¾‹å­åŒ…æ‹¬ï¼šç§‘ç ”è®ºæ–‡ä¹‹é—´çš„å¼•æ–‡å›¾ï¼Œç™¾ç§‘å…¨ä¹¦ä¸­çš„å‚è€ƒæ–‡çŒ®ç­‰ã€‚

### What does the graph of the web look like

### ç½‘ç»œå›¾æ˜¯ä»€ä¹ˆæ ·çš„

In the year 2000, the founders of AltaVista ran [an experiment](https://www.sciencedirect.com/science/article/abs/pii/S1389128600000839) to explore what the Web looked like. They asked the question: given a node $$v$$: what are the nodes $$v$$ can reach? What other nodes can reach $$v$$?

2000å¹´ï¼ŒAltaVistaçš„åˆ›å§‹äººè¿›è¡Œäº†ä¸€é¡¹å®éªŒä»¥æ¢ç´¢Webé•¿ä»€ä¹ˆæ ·ã€‚ ä»–ä»¬æå‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šç»™å®šä¸€ä¸ªèŠ‚ç‚¹$$ v $$ï¼š$$ v $$å¯ä»¥åˆ°è¾¾å“ªäº›èŠ‚ç‚¹ï¼Ÿ è¿˜æœ‰å“ªäº›å…¶ä»–èŠ‚ç‚¹å¯ä»¥è¾¾åˆ°$$ v $$ï¼Ÿ

This produced two sets of nodes:

è¿™äº§ç”Ÿäº†ä¸¤ä¸ªèŠ‚ç‚¹é›†ï¼š
$$
In(v) = \{ w| w \textrm{ can reach } v\}\\
Out(v) =\{ w| v \textrm{ can reach } w\}
$$

These sets can be ascertained by running a simple BFS. For example, in the following graph:

è¿™äº›é›†åˆå¯ä»¥é€šè¿‡è¿è¡Œä¸€ä¸ªç®€å•çš„BFSæ¥è®¡ç®—å¾—åˆ°ã€‚ ä¾‹å¦‚ï¼Œåœ¨ä¸‹å›¾ä¸­ï¼š



![BFS](../assets/img/BFS.png?style=centerme)

$$
In(A) = {A,B,C,E,G}\\
Out(A) = {A,B,C,D,F}
$$

### More on directed graphs

There are two types of directed graphs:
- <b>Strongly connected graphs</b>: A graph where any node can reach any other node. 
- <b>å¼ºè¿é€šçš„å›¾</b>ï¼šä»»ä½•èŠ‚ç‚¹éƒ½å¯ä»¥åˆ°è¾¾ä»»ä½•å…¶ä»–èŠ‚ç‚¹çš„å›¾ã€‚
- <b>Directed Acyclic Graph (DAG)</b>: A graph where there are no cycles and if $$u$$ can reach $$v$$, then $$v$$ cannot reach $$u$$
- <b>æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰</b>ï¼šæ²¡æœ‰ç¯çš„å›¾ï¼Œå¦‚æœ$$ u $$å¯ä»¥è¾¾åˆ°$$ v $$ï¼Œåˆ™$$ v $$ä¸èƒ½è¾¾åˆ°$$ u $$

Any directed graph can be represented as a combination of these two types. To do so: 

ä»»ä½•æœ‰å‘å›¾éƒ½å¯ä»¥è¡¨ç¤ºä¸ºè¿™ä¸¤ç§ç±»å‹çš„ç»„åˆã€‚ä¸ºæ­¤ï¼š

1. <b>Identify the strongly connected components (SCCs) within a directed graph</b>: An SCC is a set of nodes $$\textbf{S}$$ in a graph $$\textbf{G}$$ that is strongly connected and that there is no larger set in $$\textbf{G}$$ containing $$\textbf{S}$$ which is also strongly connected. SCCs can be identified by running a BFS on all in-links into a given node and another BFS on all the out-links from the same node and then calculating the intersection over these two sets.
2. <b>è¯†åˆ«æœ‰å‘å›¾å†…çš„å¼ºè¿æ¥ç»„ä»¶ï¼ˆSCCï¼‰</b>ï¼šSCCæ˜¯å›¾$$ \textbf {G} $$ä¸­çš„ä¸€ç»„å¼ºè¿é€šèŠ‚ç‚¹$$ \textbf {S} $$ï¼Œå¹¶ä¸”$$ \textbf {G} $$ä¸­æ²¡æœ‰æ›´å¤§çš„åŒ…å«$$ \textbf {S} $$çš„å¼ºè¿é€šé›†åˆã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ¥è¯†åˆ«SCCï¼šåœ¨ç»™å®šèŠ‚ç‚¹çš„æ‰€æœ‰å…¥è¾¹è¿è¡ŒBFSï¼Œå¹¶åœ¨åŒä¸€èŠ‚ç‚¹çš„æ‰€æœ‰å‡ºè¾¹ä¸­è¿è¡Œå¦ä¸€ä¸ªBFSï¼Œç„¶åè®¡ç®—è¿™ä¸¤ä¸ªé›†åˆçš„äº¤é›†ã€‚
3. <b>Merge the SCCs into supernodes, creating a new graph G'</b>: Create an edge between the nodes of G' if there is an edge between corresponding SCCs in G.
4. <b>å°†SCCåˆå¹¶åˆ°è¶…èŠ‚ç‚¹ä¸­ï¼Œåˆ›å»ºæ–°å›¾G'</b>ï¼šå¦‚æœGä¸­ç›¸åº”SCCä¹‹é—´å­˜åœ¨è¾¹ï¼Œåˆ™åœ¨G'çš„è¶…èŠ‚ç‚¹ä¹‹é—´åˆ›å»ºè¾¹ã€‚
5. G' is now a DAG
6. ç›´åˆ°G'æ˜¯DAG

![SCC_DAG](../assets/img/SCC_DAG.png?style=centerme)

### Bowtie structure of the web graph

ç½‘ç»œå›¾çš„Bowtieç»“æ„

Broder et al. (1999) took a large snapshot of the web and tried to understand how the SCCs in the web graph fit together as a DAG

Broderç­‰ï¼ˆ1999ï¼‰å¯¹ç½‘ç»œè¿›è¡Œäº†å¤§å¿«ç…§ï¼Œå¹¶è¯•å›¾äº†è§£ç½‘ç»œå›¾ä¸­çš„SCCå¦‚ä½•ä½œä¸ºDAGç»„åˆåœ¨ä¸€èµ·

-	Their findings are presented in the figure below. Here the starting nodes are sorted by the number of nodes that BFS visits when starting from that node
-	ä»–ä»¬çš„å‘ç°å¦‚ä¸‹å›¾ä¸­æ‰€ç¤ºã€‚ åœ¨è¿™é‡Œï¼ŒèŠ‚ç‚¹æŒ‰BFSä»¥è¯¥èŠ‚ç‚¹ä¸ºèµ·ç‚¹èƒ½è®¿é—®åˆ°çš„èŠ‚ç‚¹æ•°è¿›è¡Œæ’åº

![BowTie_1](../assets/img/BowTie_1.png?style=centerme)

-	If you look at the nodes on the left colored blue, they are able to reach a very small number of nodes before stopping
-	è§‚å¯Ÿå·¦è¾¹è“è‰²çš„èŠ‚ç‚¹ï¼Œå®ƒä»¬åœ¨BFSåœæ­¢ä¹‹å‰åªèƒ½åˆ°è¾¾å¾ˆå°‘æ•°é‡çš„èŠ‚ç‚¹
-	If you look at the nodes on the right colored magenta, they are able to reach a very large number of nodes before stopping
-	è§‚å¯Ÿå³ä¾§æ´‹çº¢è‰²çš„èŠ‚ç‚¹ï¼Œå®ƒä»¬å¯ä»¥åœ¨åœæ­¢ä¹‹å‰åˆ°è¾¾å¤§é‡èŠ‚ç‚¹
-	Using this, we can determine the number of nodes in the IN and OUT component of the bowtie shaped web graph shown below. 
-	è®¡ç®—å¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºçš„è´è¶ç»“å½¢ç½‘ç»œå›¾çš„INå’ŒOUTç»„ä»¶ä¸­çš„èŠ‚ç‚¹æ•°ã€‚

![BowTie_2](../assets/img/BowTie_2.png?style=centerme)

- SCC corresponds to the largest strongly connected component in the graph.
- SCCå¯¹åº”å›¾ä¸­æœ€å¤§çš„å¼ºè¿é€šç»„ä»¶ã€‚
- The IN component corresponds to nodes which have out-links to SCC but no in-links from SCC. The OUT component corresponds to nodes which have in-links from SCC but no out-links to SCC. 
- INç»„ä»¶å¯¹åº”äºå…·æœ‰åˆ°SCCçš„å‡ºé“¾æ¥ä½†æ²¡æœ‰æ¥è‡ªSCCçš„å…¥é“¾æ¥çš„èŠ‚ç‚¹ã€‚ OUTç»„ä»¶å¯¹åº”äºå…·æœ‰æ¥è‡ªSCCçš„å…¥é“¾æ¥ä½†æ²¡æœ‰åˆ°SCCçš„å‡ºé“¾æ¥çš„èŠ‚ç‚¹ã€‚
- Nodes with both in and out links to the SCC fall in SCC.
- åŒæ—¶å…·æœ‰åˆ°SCCå…¥å’Œå‡ºé“¾æ¥çš„èŠ‚ç‚¹å±äºSCCã€‚
- Tendrils correspond to edges going out from IN, or to edges going into OUT. TUBES are connections from IN to OUT that bypass SCC.
- å·é¡»å¯¹åº”äºä»INä¼¸å‡ºçš„è¾¹ï¼Œæˆ–å¯¹åº”äºè¿›å…¥OUTçš„è¾¹ã€‚TUBESæ˜¯ä»INåˆ°OUTçš„ç»•è¿‡SCCçš„è¿æ¥ã€‚
-	Disconnected components are not connected to SCC at all
-	å­¤ç«‹ç»„ä»¶å’Œä¸Šé¢çš„å‡ ç±»èŠ‚ç‚¹éƒ½æ²¡æœ‰è¿æ¥

## PageRank - Ranking nodes on the graph

Not all pages on the web are equal. When we run a query, we donâ€™t want to find all the pages that contain query words. Instead, we want to learn how we rank the pages. How can we decide on the importance of pages based on the link structure of the web graph?

å¹¶éç½‘ç»œä¸Šçš„æ‰€æœ‰é¡µé¢éƒ½åŒæ ·é‡è¦ã€‚å½“è¿è¡ŒæŸ¥è¯¢æ—¶ï¼Œæˆ‘ä»¬ä¸æƒ³æ‰¾åˆ°æ‰€æœ‰åŒ…å«æŸ¥è¯¢è¯çš„é¡µé¢ã€‚ç›¸åï¼Œæˆ‘ä»¬æƒ³å­¦ä¹ å¦‚ä½•å¯¹é¡µé¢è¿›è¡Œæ’åã€‚é‚£ä¹ˆå¦‚ä½•æ ¹æ®ç½‘ç»œå›¾çš„é“¾æ¥ç»“æ„ç¡®å®šæŸä¸ªé¡µé¢çš„é‡è¦æ€§ï¼Ÿ

<b>The main idea here is that links are votes</b>

<b>è¿™é‡Œçš„ä¸»è¦æ€æƒ³æ˜¯é“¾æ¥ä¹Ÿæ˜¯æŠ•ç¥¨</b>

In-links into a page are counted as votes for that page which helps determine the importance of the page. In-links from important pages count more. This turns into a recursive relationship: page A's importance depends on page B, whose importance depends on page C and so on.

è¿›å…¥é¡µé¢çš„é“¾æ¥è¢«è§†ä¸ºç»™è¯¥é¡µé¢çš„æŠ•ç¥¨ï¼Œè¿™æœ‰åŠ©äºç¡®å®šè¯¥é¡µé¢çš„é‡è¦æ€§ã€‚æ›´é‡è¦çš„é¡µé¢çš„in-linksæƒå€¼æ›´é«˜ã€‚è¿™å˜æˆäº†ä¸€ç§é€’å½’å…³ç³»ï¼šé¡µé¢Açš„é‡è¦æ€§å–å†³äºé¡µé¢Bï¼Œè€Œé¡µé¢Bçš„é‡è¦æ€§å–å†³äºé¡µé¢Cï¼Œä¾æ­¤ç±»æ¨ã€‚

Each link's vote is proportional to the importance of its source page. In the figure below, node $$j$$ has links from $$i$$ and $$k$$ which each contribute importance values equal to $$\frac{r_i}{3}$$ and $$\frac{r_k}{4}$$. This is because there are three out-links from node $$i$$ and four out-links from node $$k$$. Similarly, node $$j$$'s importance is equally distributed between the three out-links that it shares with other nodes.

æ¯ä¸ªé“¾æ¥çš„æŠ•ç¥¨æƒé‡ä¸å…¶æºç½‘é¡µçš„é‡è¦æ€§æˆæ­£æ¯”ã€‚åœ¨ä¸‹å›¾ä¸­ï¼ŒèŠ‚ç‚¹$$ j $$å…·æœ‰$$ i $$å’Œ$$ k $$çš„é“¾æ¥ï¼Œæ¯ä¸ªé“¾æ¥çš„é‡è¦æ€§å€¼ç­‰äº$$ \frac {r_i} {3} $$å’Œ$$ \frac { r_k} {4} $$ã€‚è¿™æ˜¯å› ä¸ºèŠ‚ç‚¹$$ i $$æœ‰ä¸‰ä¸ªå‡ºé“¾æ¥ï¼ŒèŠ‚ç‚¹$$ k $$æœ‰å››ä¸ªå‡ºé“¾æ¥ã€‚åŒæ ·ï¼ŒèŠ‚ç‚¹$$ j $$çš„é‡è¦æ€§å¹³å‡åˆ†é…ç»™äº†å®ƒçš„ä¸‰ä¸ªå‡ºé“¾æ¥èŠ‚ç‚¹ã€‚

![LinkWeights](../assets/img/LinkWeights.png?style=centerme)

To summarize, a page is important if it is pointed to by other important pages. Given a set of pages, we can define the page rank $$r_j$$ for node $$j$$ as:

æ€»è€Œè¨€ä¹‹ï¼Œå¦‚æœå…¶ä»–é‡è¦çš„ç½‘é¡µæŒ‡å‘è¯¥ç½‘é¡µï¼Œåˆ™è¯¥ç½‘é¡µå¾ˆé‡è¦ã€‚ ç»™å®šä¸€ç»„ç½‘é¡µï¼Œæˆ‘ä»¬å¯ä»¥å°†èŠ‚ç‚¹$$ j $$çš„ç½‘é¡µæ’åå¾—åˆ†$$ r_j $$å®šä¹‰ä¸ºï¼š
$$
\textbf{r}_j = \sum_{i\rightarrow j}\frac{\textbf{r}_i}{d_i} 
$$

where $$i$$ refers to every node that has an outgoing edge into $$j$$ and $$d_i$$ is the out degree of node i.

å…¶ä¸­$$ i $$æŒ‡çš„æ˜¯æœ‰å‡ºè¿æ¥åˆ°$$ j $$çš„èŠ‚ç‚¹ï¼Œè€Œ$$ d_i $$æ˜¯èŠ‚ç‚¹içš„å‡ºåº¦ã€‚

### Matrix formulationçŸ©é˜µå½¢å¼
We can formulate these relationships as $$N$$ PageRank equations using $$N$$ variables. We could use Gaussian elimination to solve but this would take a very long time for large graphs such as the web graph. 

æˆ‘ä»¬å¯ä»¥ç”¨$$ N $$ä¸ªå˜é‡å°†è¿™äº›å…³ç³»å…¬å¼åŒ–ä¸º$$ N $$ ä¸ªPageRankæ–¹ç¨‹ã€‚ä½¿ç”¨é«˜æ–¯æ¶ˆå»æ³•æ±‚è§£ï¼Œä½†æ˜¯å¯¹äºå¤§å‹å›¾ï¼ˆä¾‹å¦‚ç½‘ç»œå›¾ï¼‰ï¼Œè¿™å°†èŠ±è´¹å¾ˆé•¿æ—¶é—´ã€‚

Instead, we can represent the graph as an adjacency matrix $$\textbf{M}$$ that is column stochastic. This means that all the columns must add up to 1. So for node $$j$$ all the entries, in column $$j$$ will sum to 1. 

ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥å°†å›¾è¡¨ç¤ºä¸ºåˆ—éšæœºçš„é‚»æ¥çŸ©é˜µ$$ \textbf {M} $$ã€‚ è¿™æ„å‘³ç€æ‰€æœ‰åˆ—çš„æ€»å’Œå¿…é¡»ä¸º1ã€‚å› æ­¤ï¼Œå¯¹äºèŠ‚ç‚¹$$ j $$ï¼Œæ‰€æœ‰æ¡ç›®åœ¨ç¬¬$ j $åˆ—ä¸­çš„æ€»å’Œä¸º1ã€‚
$$
\textrm{If } j \rightarrow i \textrm{ , then } \textbf{M}_{ij} = \frac{1}{d_j}\\
$$

$$r_i$$ is the importance score of page $$i$$

$$r_i$$ æ˜¯é¡µé¢$$i$$çš„é‡è¦æ€§åˆ†æ•°ï¼Œä¸€å¼€å§‹
$$
\sum_i \textbf{r}_i = 1
$$

This will fulfill the requirement that the importance of every node must sum to 1. We now rewrite the PageRank vector as the following matrix equation:

è¿™å°†æ»¡è¶³æ¯ä¸ªèŠ‚ç‚¹åˆå§‹æ—¶çš„é‡è¦æ€§æ€»å’Œå¿…é¡»ä¸º1çš„è¦æ±‚ã€‚æˆ‘ä»¬ç°åœ¨å°†PageRankå‘é‡é‡å†™ä¸ºä»¥ä¸‹çŸ©é˜µæ–¹ç¨‹å¼ï¼š
$$
\textbf{r} = \textbf{M}.\textbf{r}
$$

In this case, the PageRank vector will be the eigenvector of the stochastic web matrix $$\textbf{M}$$ that corresponds to the eigenvalue of 1. 

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒPageRankå‘é‡å°†æ˜¯å¯¹åº”ç‰¹å¾å€¼ä¸º1çš„éšæœºç½‘ç»œçŸ©é˜µ$$ \textbf {M} $$çš„ç‰¹å¾å‘é‡ã€‚

### Random walk formulation

éšç€$$ t $$æ¥è¿‘æ— ç©·å¤§ï¼Œéšæœºæ¸¸èµ°å°†è¾¾åˆ°ç¨³å®šçŠ¶æ€ï¼š

$$
pï¼ˆt + 1ï¼‰= \ textbf {M} \ cdot pï¼ˆtï¼‰= pï¼ˆtï¼‰
$$

å½“æˆ‘ä»¬æ±‚è§£æ–¹ç¨‹å¼$$ \ textbf {r} = \ textbf {M} \ cdot \ textbf {r} $$; $$ \ textbf {r} $$å®é™…ä¸Šåªæ˜¯å½“$$ t $$æ¥è¿‘æ— ç©·å¤§æ—¶ï¼Œè¯¥å†²æµªè€…åœ¨æ—¶é—´$$ \ textbf {t} $$å¤„çš„æ¦‚ç‡åˆ†å¸ƒã€‚ å®ƒåœ¨å›¾å½¢ä¸Šæ¨¡æ‹Ÿäº†è¯¥éšæœºæ²ƒå…‹è¿‡ç¨‹çš„å¹³ç¨³åˆ†å¸ƒã€‚

PageRank relations are very related to random walks. Imagine a web graph and a random surfer that surfs the graph. At any time $$t$$, the surfer is at any page $$i$$. The surfer will then select any outgoing link uniformly at random and make a new step at time $$t+1$$, and this process continues infinitely. Let $$p(t)$$ be the vector whose $$i$$th component is the probability that a surfer is at page $$i$$ at time $$t$$. $$p(t)$$ is then a probability distribution over pages at a given time $$t$$.

PageRankä¸éšæœºæ¸¸èµ°éå¸¸ç›¸å…³ã€‚æƒ³è±¡ä¸€ä¸ªç½‘ç»œå›¾å’Œæµè§ˆè¯¥å›¾çš„éšæœºç½‘ä¸Šå†²æµªè€…ã€‚åœ¨ä»»æ„æ—¶é—´$$ t $$ï¼Œå†²æµªè€…éƒ½åœ¨ä»»æ„ç½‘é¡µ$$ i $$ã€‚ ç„¶åï¼Œå†²æµªè€…å°†éšæœºåœ°é€‰æ‹©ä¸€ä¸ªå‡ºé“¾æ¥ï¼Œå¹¶åœ¨$ t + 1 $æ—¶åˆ»æµè§ˆä¸‹ä¸€ä¸ªç½‘é¡µï¼Œå¹¶ä¸”æ­¤è¿‡ç¨‹å°†æ— é™æœŸåœ°ç»§ç»­ã€‚è®°å‘é‡$$p(t)$$ï¼Œå…¶ç¬¬$$ i $$ä¸ªåˆ†é‡æ˜¯å†²æµªè€…åœ¨æ—¶é—´$ t $å¤„åœ¨é¡µé¢$ i $çš„æ¦‚ç‡ã€‚ é‚£ä¹ˆï¼Œ$$p(t)$$æ˜¯åœ¨ç»™å®šæ—¶é—´$ t $æ—¶å†²æµªè€…æ‰€åœ¨é¡µé¢çš„æ¦‚ç‡åˆ†å¸ƒã€‚
$$
p(t+1) = \textbf{M}\cdot p(t) 
$$

As $$t$$ approaches infinity, the random walk will reach a steady state:

éšç€$$ t $$è¶‹äºæ— ç©·å¤§ï¼Œéšæœºæ¸¸èµ°å°†è¾¾åˆ°ç¨³å®šçŠ¶æ€ï¼š
$$
p(t+1) = \textbf{M}\cdot p(t) = p(t)
$$

When we solve the equation $$\textbf{r}=\textbf{M}\cdot \textbf{r}$$; $$\textbf{r}$$ is really just the probability distribution of where this surfer will be at time $$\textbf{t}$$, when $$t$$ approaches infinity. Itâ€™s modeling the stationary distribution of this random walker process on the graph.

å½“æˆ‘ä»¬æ±‚è§£æ–¹ç¨‹ $$\textbf{r}=\textbf{M}\cdot \textbf{r}$$æ—¶; $$ \textbf {r} $$å®é™…ä¸Šåªæ˜¯å½“$$ t $$æ¥è¿‘æ— ç©·å¤§æ—¶ï¼Œè¯¥å†²æµªè€…åœ¨æ—¶é—´$$ \textbf {t} $$æ—¶å¤„åœ¨çš„é¡µé¢çš„æ¦‚ç‡åˆ†å¸ƒã€‚ å®ƒåœ¨å›¾ä¸Šæ¨¡æ‹Ÿäº†è¯¥éšæœºæ¸¸èµ°è¿‡ç¨‹çš„å¹³ç¨³åˆ†å¸ƒã€‚

### Power law iterationå¹‚å¾‹è¿­ä»£

Starting from any vector $$u$$, the limit $$\textbf{M}(\textbf{M}(â€¦ \textbf{M}(\textbf{M} \textbf{u})))$$ is the long-term distribution of the surfers. In other words, r is the limit when we take a vector $$u$$ and multiply with $$\textbf{M}$$ long enough. This means that we can efficiently solve for $$\textbf{r}$$ using power iterations. 

ä»ä»»æ„å‘é‡$$ u $$å¼€å§‹ï¼Œæé™ $$\textbf{M}(\textbf{M}(â€¦ \textbf{M}(\textbf{M} \textbf{u})))$$æ˜¯å†²æµªè€…æµè§ˆçš„ç½‘é¡µè¾¾åˆ°ç¨³å®šæ—¶çš„åˆ†å¸ƒã€‚æ¢å¥è¯è¯´ï¼Œræ˜¯æˆ‘ä»¬å–ä¸€ä¸ªå‘é‡$$ u $$å¹¶ä¹˜ä»¥$$ \textbf {M} $$è¶³å¤Ÿå¤šæ¬¡æ—¶çš„æé™ã€‚ è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¹‚å¾‹è¿­ä»£æœ‰æ•ˆåœ°æ±‚è§£$$ \textbf {r} $$ã€‚

<b>Limiting distribution = principal eigenvector of $\textbf{M}$ = PageRank</b>

<b>æé™åˆ†å¸ƒ= $ \textbf {M} $çš„ä¸»è¦ç‰¹å¾å‘é‡= PageRank </b>

Initialize: $$ \textbf{r}^{(0)} = [\frac{1}{N},...,\frac{1}{N}]^T$$\\
Iterate: $$\textbf{r}^{(t+1)} = \textbf{M}.\textbf{r}^{(t)}$$\\
Stop when: $$|\textbf{r}^{(t+1)} - \textbf{r}^{(t)} | < \epsilon $$

We keep iterating until we converge based on epsilon. In practice, this tends to converge within 50-100 iterations. 

æˆ‘ä»¬ä¸æ–­è¿­ä»£ï¼Œç›´åˆ°åŸºäºepsilonæ”¶æ•›ä¸ºæ­¢ã€‚ å®é™…æ“ä½œä¸­ï¼Œé€šå¸¸åœ¨50-100æ¬¡è¿­ä»£ä¸­æ”¶æ•›ã€‚

### Example

![PRExample1](../assets/img/PRExample1.png?style=centerme)
![PRExample2](../assets/img/PRExample2.png?style=centerme)

The $$\textbf{r}$$ vector you are left with is the page rank or page importances. So page $$y$$ has importance $$\frac{6}{15}$$, page $$a$$ has importance $$\frac{6}{15}$$ and page $$m$$ has importance $$\frac{3}{15}$$

æœ€åçš„$$ \textbf {r} $$å‘é‡æ˜¯é¡µé¢æ’åæˆ–é¡µé¢é‡è¦æ€§ã€‚ å› æ­¤ï¼Œé¡µé¢$$ y $$å…·æœ‰é‡è¦æ€§$$ \frac {6} {15} $$ï¼Œé¡µé¢$$ a $$å…·æœ‰é‡è¦æ€§$$ \frac {6} {15} $$ï¼Œé¡µé¢$$ m $$å…·æœ‰é‡è¦æ€§$$\frac{3}{15}$$


### PageRank: Problems

1. <b>Dead ends</b>: These are pages that have in-links but no out-links. As a random surfer, itâ€™s like coming to a cliff and having nowhere else to go. This would imply that the adjacency matrix is no longer column stochastic and will leak out importance.
2. <b>æ­»è·¯</b>ï¼šè¿™äº›é¡µé¢å…·æœ‰å…¥é“¾æ¥ï¼Œä½†æ²¡æœ‰å‡ºé“¾æ¥ã€‚ ä½œä¸ºä¸€ä¸ªéšæœºçš„å†²æµªè€…ï¼Œè¿™å°±åƒæ¥åˆ°æ‚¬å´–ä¸Šï¼Œæ— å¤„å¯å»ã€‚ è¿™å°†æ„å‘³ç€é‚»æ¥çŸ©é˜µä¸å†æ˜¯åˆ—éšæœºçš„ï¼ˆbè¿™ä¸€åˆ—çš„å’Œä¸º0ï¼Œæ²¡æœ‰å‡ºè¾¹ï¼‰ï¼Œå¹¶ä¸”ä¼šæ³„æ¼é‡è¦æ€§ã€‚
  ![DeadEnd](../assets/img/DeadEnd.png?style=centerme)
3. <b>Spider traps</b>:: These are pages with only self-edges as outgoing edges causing the surfer to get trapped. Eventually the spider trap will absorb all importance. Given a graph with a self-loop in $$b$$, the random surfer will eventually navigate to $$b$$ and get stuck in $$b$$ for the rest of the iterations. Power iteration will converge with $$b$$ having all the importance and leave $$a$$ with no importance.
4. <b>èœ˜è››é™·é˜±</b>ï¼šè¿™äº›é¡µé¢ä»…å…·æœ‰è¿å‘è‡ªå·±çš„å‡ºè¿æ¥ï¼Œä»è€Œå¯¼è‡´å†²æµªè€…è¢«å›°ä½ã€‚æœ€ç»ˆï¼Œèœ˜è››é™·é˜±å°†å¸æ”¶æ‰€æœ‰çš„é‡è¦æ€§ã€‚ç»™å®šä¸€ä¸ªå¸¦æœ‰è‡ªç¯$ b $çš„å›¾ï¼Œéšæœºå†²æµªè€…æœ€ç»ˆå°†å¯¼èˆªåˆ°$$ b $$ï¼Œå¹¶åœ¨å…¶ä½™çš„è¿­ä»£ä¸­é™·å…¥$ b $ã€‚å¹‚æ¬¡è¿­ä»£å°†æ”¶æ•›åˆ°$$ b $$å…·æœ‰äº†æ‰€æœ‰çš„é‡è¦æ€§ï¼Œè€Œä½¿$$ a $$ä¸é‡è¦ã€‚
  ![SpiderTrap](../assets/img/SpiderTrap.png?style=centerme)

<b>How do we solve this? Using random teleportation or random jumps!</b>

<b>æˆ‘ä»¬è¯¥å¦‚ä½•è§£å†³ï¼Ÿä½¿ç”¨éšæœºä¼ é€æˆ–éšæœºè·³è·ƒï¼</b>

Whenver a random walker makes a step, the surfer has two options. It can flip a coin and with probability ğœ· continue to follow the links, or with probability $$(1- ğœ·)$$ it will teleport to a different webpage. Where do you jump? to any of the nodes with equal probability. Usually ğœ· is set around 0.8 to 0.9. 

æ¯å½“éšæœºæ¸¸èµ°è€…è¿ˆå‡ºä¸€æ­¥æ—¶ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ã€‚å®ƒå¯ä»¥æ·ç¡¬å¸å¹¶ä»¥æ¦‚ç‡ğœ·ç»§ç»­è·Ÿéšé“¾æ¥èµ°ï¼Œæˆ–è€…ä»¥ $$(1- ğœ·)$$çš„æ¦‚ç‡éšæœºä¼ é€åˆ°å¦ä¸€ä¸ªç½‘é¡µã€‚é€šå¸¸ğœ·è®¾ç½®åœ¨0.8åˆ°0.9å·¦å³ã€‚

In case of a spider trap: Teleport out in a finite number of steps.\\
In case of a dead end: Teleport out with a total probability of 1. This will make the matrix column stochastic.

å¦‚æœæ˜¯èœ˜è››é™·é˜±ï¼šä¼šåœ¨æœ‰é™çš„æ­¥æ•°å†…ä¼ é€å‡ºå»ã€‚
å¦‚æœå‡ºç°æ­»è§’ï¼šä»¥æ€»æ¦‚ç‡1ä¼ é€å‡ºå»ã€‚è¿™å°†ä½¿çŸ©é˜µæ˜¯åˆ—éšæœºçš„ã€‚

Putting this together, the PageRank equation (as proposed by [Brin-Page, 98](http://snap.stanford.edu/class/cs224w-readings/Brin98Anatomy.pdf)) can be written as:

ç»¼ä¸Šæ‰€è¿°ï¼ŒPageRankæ–¹ç¨‹å¯ä»¥å†™ä¸ºï¼š
$$
r_j = \sum_{i \rightarrow j} \frac{r_i}{d_i} + (1 - ğœ·) \frac{1}{N} 
$$

We can now define the Google Matrix A and apply power iteration to solve for $$\textbf{r}$$ as before

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰Google Matrix Aå¹¶åº”ç”¨å¹‚è¿­ä»£æ¥åƒä»¥å‰ä¸€æ ·è§£å†³
$$
A = ğœ·\textbf{M} + (1 - ğœ·)[\frac{1}{N}]_{NXN}
$$

$$
\textbf{r} = \textbf{A} \cdot \textbf{r}
$$

Note: This formulation assumes that $$\textbf{M}$$ has no dead ends. We can either preprocess matrix $$\textbf{M}$$ to remove all dead ends or explicitly follow random teleport links with probability 1.0 from dead-ends.

æ³¨æ„ï¼šæ­¤å…¬å¼å‡å®š$$ \textbf {M} $$æ²¡æœ‰æ­»è§’ã€‚æˆ‘ä»¬å¯ä»¥å¯¹çŸ©é˜µ$$ \textbf {M} $$è¿›è¡Œé¢„å¤„ç†ä»¥åˆ é™¤æ‰€æœ‰æ­»è§’ï¼Œä¹Ÿå¯ä»¥åœ¨æ­»è§’ä¸­ä»¥æ¦‚ç‡1.0æ˜¾å¼åœ°éšæœºä¼ é€ã€‚

### Computing PageRank: Sparse matrix formulation

### è®¡ç®—PageRankï¼šç¨€ç–çŸ©é˜µå…¬å¼

The key step in computing page rank is the matrix-vector multiplication

è®¡ç®—page rankçš„å…³é”®æ­¥éª¤æ˜¯çŸ©é˜µå‘é‡ä¹˜æ³•
$$
\textbf{r}_{new} = \textbf{A} \cdot \textbf{r}_{old}
$$

We want to be able to iterate this as many times as possible. If $$\textbf{A}$$, $$\textbf{r}_{old}$$, $$\textbf{r}_{new}$$ are small and can fit in memory then there is no problem. But if $$N$$ = 1 billion pages and each entry is 4 bytes, then just for storing $$\textbf{r}_{old}$$ and $$\textbf{r}_{new}$$, we would need 8GB of memory. Matrix A would have $$N^2 = 10^{18}$$ entries, which would require close to 10 million GB of memory! 

æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¯¹æ­¤è¿›è¡Œå°½å¯èƒ½å¤šçš„è¿­ä»£ã€‚å¦‚æœ$$ \textbf {A} $$ï¼Œ$$ \textbf{r}_ {old} $$ï¼Œ$$\textbf {r} _ {new} $$å¾ˆå°å¹¶ä¸”å¯ä»¥å®¹çº³åœ¨å†…å­˜ä¸­ï¼Œé‚£ä¹ˆå°±æ²¡æœ‰é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå¦‚æœ$$ N $$ = 10äº¿é¡µå¹¶ä¸”æ¯ä¸ªæ¡ç›®éƒ½æ˜¯4ä¸ªå­—èŠ‚ï¼Œé‚£ä¹ˆä»…ç”¨äºå­˜å‚¨$$ \textbf {r} _ {old} $$å’Œ$$ \textbf {r} _ {new} $$ï¼Œæˆ‘ä»¬å°†éœ€è¦8GBçš„å†…å­˜ã€‚çŸ©é˜µAå°†æœ‰$$ N ^ 2 = 10 ^ {18} $$ä¸ªæ¡ç›®ï¼Œè¿™å°†éœ€è¦è¿‘1000ä¸‡GBçš„å†…å­˜ï¼

We can rearrange the computation to look like this:

æˆ‘ä»¬å¯ä»¥é‡æ–°ç»„ç»‡ä¸€ä¸‹è®¡ç®—ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
$$
\textbf{r} = ğœ· \textbf{M} \cdot \textbf{r} + \frac{[ğŸ âˆ’ ğœ·]}{ğ‘µ}
$$

This is easier to compute because $$\textbf{M}$$ is a sparse matrix, multiplying it with a scalar is still sparse, and then multiplying it with a vector is not as computationally intensive. After this, we simply add a constant which is the probability of the random walker directly jumping to $$\textbf{r}_{new}$$. The amount of memory that we now need goes down from $$O(N^2)$$ to $$O(N)$$. At every iteration, some of the pagerank can leak out and by renormalizing M we can re-insert the leaked page rank.

è¿™å¾ˆå®¹æ˜“è®¡ç®—ï¼Œå› ä¸º$$ \textbf {M} $$æ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼Œå°†å®ƒä¸æ ‡é‡ç›¸ä¹˜ä»ç„¶æ˜¯ç¨€ç–çš„ï¼Œç„¶åå†å°†å…¶ä¸å‘é‡ç›¸ä¹˜å°±ä¸é‚£ä¹ˆè®¡ç®—å¯†é›†ã€‚æ­¤åï¼Œæˆ‘ä»¬åªéœ€æ·»åŠ ä¸€ä¸ªå¸¸æ•°ï¼Œè¯¥å¸¸æ•°å°±æ˜¯éšæœºæ¸¸èµ°è€…ç›´æ¥è·³åˆ°$$ \textbf {r} _ {new} $$çš„æ¦‚ç‡ã€‚æˆ‘ä»¬ç°åœ¨éœ€è¦çš„å†…å­˜é‡ä»$$O(N^2)$$é™è‡³$$O(N)$$ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼ŒæŸäº›é¡µé¢æ’åå¯èƒ½ä¼šæ³„æ¼å‡ºå»ï¼Œè¿™æ—¶é€šè¿‡å¯¹Mé‡æ–°è¿›è¡Œè§„èŒƒåŒ–ï¼Œå°±å¯ä»¥é‡æ–°æ’å…¥æ³„æ¼çš„page rankã€‚

Here is an example of how PageRank would work if applied it to a graph:
![PRGraphExample](../assets/img/PRGraphExample.png?style=centerme)

In the figure above, within each node is its pagerank score or importance score. Scores sum to 100, size of the node is proportional to its score.  Node B has very high importance because a lot of nodes point to it. These nodes still have importance without in-links because random walker jump can jump to them. Node C has only one-link but since its from B it also becomes very important. However, C's importance is still less than B because B has a lot of other in-links going into it.

åœ¨ä¸Šå›¾ä¸­ï¼Œæ¯ä¸ªèŠ‚ç‚¹å†…æ˜¯å…¶page rankå¾—åˆ†æˆ–é‡è¦æ€§å¾—åˆ†ã€‚åˆ†æ•°æ€»å’Œä¸º100ï¼ŒèŠ‚ç‚¹çš„å¤§å°ä¸å…¶åˆ†æ•°æˆæ­£æ¯”ã€‚èŠ‚ç‚¹Bå…·æœ‰éå¸¸é«˜çš„é‡è¦æ€§ï¼Œå› ä¸ºå¾ˆå¤šèŠ‚ç‚¹éƒ½æŒ‡å‘å®ƒã€‚è¿™äº›èŠ‚ç‚¹åœ¨æ²¡æœ‰å…¥é“¾æ¥çš„æƒ…å†µä¸‹ä»ç„¶æœ‰é‡è¦æ€§ï¼Œå› ä¸ºéšæœºæ­¥è¡Œè€…å¯ä»¥é€šè¿‡éšæœºTPè·³è·ƒåˆ°å®ƒä»¬ã€‚èŠ‚ç‚¹Cåªæœ‰ä¸€ä¸ªå…¥é“¾æ¥ï¼Œä½†æ˜¯ç”±äºå®ƒæ¥è‡ªBï¼Œå› æ­¤å®ƒä¹Ÿå˜å¾—éå¸¸é‡è¦ã€‚ ä½†æ˜¯ï¼ŒCçš„é‡è¦æ€§ä»ç„¶ä¸å¦‚Bï¼Œå› ä¸ºBè¿˜æœ‰å¾ˆå¤šå…¶ä»–å…¥é“¾æ¥ã€‚

### PageRank Algorithm

Input: 
- Directed graph G (can have spider traps and dead ends)
- Parameter ğœ·

Output: 
- PageRank vector $$r^{new}$$

Set: 
$$r_j^{old} = \frac{1}{N}$$:

Repeat until convergence: 

$$
\sum_j|r_j^{new} - r_j^{old}| < \epsilon
$$

$$
\forall j: r_j^{'new} = \sum_{i \rightarrow j}ğœ· \frac{r_i^{old}}{d_i}\\
	   r_j^{'new} = 0 \textrm{ if in-degree of j is } 0
$$

Now re-insert the leaked PageRank:

$$
\forall j: r_j^{new} = r_j^{'new} + \frac{1-s}{N} \textrm{ where } S = \sum_jr_j^{'new}\\
r^{old} = r^{new}
$$


## Personalized PageRank and random walk with restarts

Imagine we have a bipartite graph consisting of users on one side (circles in the figure below) and items on the other (squares). We would like to ask how related two items are or how related two users are. Given that the users have already purchased in the past - what can we recommend to them, based on what they have in common with other users.  

æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªäºŒéƒ¨å›¾ï¼Œå®ƒç”±ç”¨æˆ·ï¼ˆä¸‹å›¾ä¸­çš„åœ†åœˆï¼‰å’Œé¡¹ç›®ï¼ˆæ­£æ–¹å½¢ï¼‰ç»„æˆã€‚ æˆ‘ä»¬æƒ³é—®ä¸¤ä¸ªé¡¹ç›®ä¹‹é—´çš„ç›¸å…³æ€§æˆ–ä¸¤ä¸ªç”¨æˆ·ä¹‹é—´çš„ç›¸å…³æ€§ã€‚ é‰´äºç”¨æˆ·è¿‡å»å·²ç»è´­ä¹°è¿‡æˆ–æ ¹æ®ä»–ä»¬ä¸å…¶ä»–ç”¨æˆ·çš„å…±åŒç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å‘ä»–ä»¬æ¨èä»€ä¹ˆï¼Ÿ

![ItemProduct](../assets/img/ItemProduct.png?style=centerme)

We can use different metrics to quantify this such as shortest path or number of common neighbors, however these are not very versatile. Instead, we can use a modified version of PageRank that doesn't rank all pages by importance rather it ranks them by proximity to a given set. This set is called the teleport set $$\textbf{S}$$ and this method is called Personalized PageRank. 

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„æŒ‡æ ‡æ¥é‡åŒ–ï¼Œä¾‹å¦‚æœ€çŸ­è·¯å¾„æˆ–å…±åŒé‚»å±…çš„æ•°é‡ï¼Œä½†æ˜¯è¿™äº›åŠŸèƒ½ä¸æ˜¯å¾ˆé€šç”¨ã€‚ ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨PageRankçš„ä¿®æ”¹ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬ä¸ä¼šæŒ‰é‡è¦æ€§å¯¹æ‰€æœ‰é¡µé¢è¿›è¡Œæ’åï¼Œè€Œæ˜¯æ ¹æ®ä¸ç»™å®šé›†åˆçš„æ¥è¿‘ç¨‹åº¦å¯¹å®ƒä»¬è¿›è¡Œæ’åã€‚ æ­¤é›†ç§°ä¸ºä¼ é€é›†$$ \textbf {S} $$ï¼Œæ­¤æ–¹æ³•ç§°ä¸ºâ€œä¸ªæ€§åŒ–çš„PageRankâ€ã€‚

One way to implement this is to take the teleport set and compute the pagerank vector using power iteration. However, in this case, since we only have a single node in S, its quicker to just do a simple random walk. So, the random walker starts at node $$\textbf{Q}$$ and then whenever it teleports it goes back to $$\textbf{Q}$$. This will give us all the nodes that are most similar to $$\textbf{Q}$$ by identifying those with the highest visit counts. We thus achieve a very simple recommender system that works very well in practice, and we can call it random walk with restarts.

ä¸€ç§å®ç°æ–¹æ³•æ˜¯å¯¹ä¼ é€é›†ä½¿ç”¨å¹‚å¾‹è¿­ä»£æ¥è®¡ç®—pagerankå‘é‡ã€‚ ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”±äºSä¸­åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå› æ­¤æ‰§è¡Œç®€å•çš„éšæœºæ¸¸èµ°ä¼šæ›´å¿«ã€‚ å› æ­¤ï¼Œéšæœºæ¸¸èµ°è€…ä»èŠ‚ç‚¹$$ \textbf {Q} $$å¼€å§‹ï¼Œç„¶åæ¯å½“å®ƒTPæ—¶ï¼Œå®ƒå°±ä¼šå›åˆ°$$ \textbf {Q} $$ã€‚è®¿é—®é‡æœ€é«˜çš„èŠ‚ç‚¹å³ä¸ºä¸$$ \textbf {Q} $$æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬è·å¾—äº†ä¸€ä¸ªéå¸¸ç®€å•çš„æ¨èç³»ç»Ÿï¼Œè¯¥ç³»ç»Ÿåœ¨å®è·µä¸­æ•ˆæœå¾ˆå¥½ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯ä»¥å°†å…¶ç§°ä¸ºå¸¦é‡å¯çš„éšæœºæ¸¸èµ°ã€‚

![QPPR](../assets/img/QPPR.png?style=centerme)

Random walk with restarts is able to account for

é‡æ–°å¯åŠ¨çš„éšæœºæ¸¸èµ°èƒ½å¤ŸåŒ…å«

- Multiple connections
- Multiple paths
- Direct and indirect connections
- Degree of the node
- å¤šè¿æ¥
- å¤šè·¯å¾„
- ç›´æ¥å’Œé—´æ¥è¿æ¥
- èŠ‚ç‚¹çš„åº¦


## PageRank Summary:

- <b>Normal pagerank</b>:
Teleportation vector is uniform

- <b>å¸¸è§„pagerank</b>ï¼š

  éšæœºè·³è·ƒå‘é‡æ˜¯å‡åŒ€çš„

- <b>Personalized PageRank</b>: 
Teleport to a topic specific set of pages. 
Nodes can have different probabilities of surfer landing there

- <b>ä¸ªæ€§åŒ–PageRank </b>ï¼š

  ä¼ é€åˆ°ç‰¹å®šä¸»é¢˜çš„é¡µé¢é›†ã€‚

  éšæœºä¼ é€åˆ°çš„èŠ‚ç‚¹å¯èƒ½æœ‰ä¸åŒçš„æ¦‚ç‡

- <b>Random walk with restarts</b>:

- å¸¦æœ‰é‡æ–°å¯åŠ¨çš„éšæœºæ¸¸èµ°
Topic specific pagerank where teleport is always to the same node. In this case, we don't need power iteration we can just use random walk and its very fast and easy
  
  ç‰¹å®šäºä¸»é¢˜çš„pagerankï¼Œå…¶ä¸­ä¼ é€å§‹ç»ˆæ˜¯åˆ°åŒä¸€èŠ‚ç‚¹çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¸éœ€è¦å¹‚æ¬¡è¿­ä»£ï¼Œä½¿ç”¨å¿«é€Ÿç®€å•çš„éšæœºæ¸¸èµ°å³å¯ã€‚
